{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "081b1cd9",
   "metadata": {},
   "source": [
    "### 5.8: Recurrent Neural Networks (RNNs) & LSTMs for Text\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- **Model Architecture:** A character-level LSTM recurrent neural network using TensorFlow and Keras, trained to predict the next character in a sequence from Shakespeare's works. This enables text generation that mimics Shakespeare's style, though results vary with training duration and data preprocessing.\n",
    "- **Dataset Handling:** Used the provided Tiny Shakespeare dataset (~1 million characters), cleaned by replacing newlines with spaces (as per your code), leading to prose-like output rather than verse. Research suggests preserving structure improves poetic rhythm, but this follows your setup.\n",
    "- **Training and Evaluation:** Split into 90% train/10% test; trained for 20 epochs with categorical cross-entropy loss and accuracy metrics. Typical final test loss is around 1.5–2.0 (perplexity 4–7), indicating moderate predictive power—coherent words and phrases emerge, but full sentences may lack depth without more epochs or data.\n",
    "- **Generation:** The model generates samples starting from a seed phrase, using temperature sampling for creativity. It seems likely that outputs will capture Shakespearean vocabulary and patterns, though they may include invented words or inconsistencies due to the character's simplicity.\n",
    "- **Performance Insights:** Evidence leans toward solid learning of short-term dependencies (e.g., spelling, basic grammar), but long-range context (e.g., plot coherence) is limited in small models. Overfitting is common; dropout helps mitigate this.\n",
    "\n",
    "#### Implementation Steps\n",
    "\n",
    "1. **Data Preparation:** Convert text to integer sequences, create input-target pairs (sequence length 100), and split into train/test.\n",
    "2. **Model Building:** Embedding layer for character vectors, two LSTM layers for sequence modeling, dense output for predictions.\n",
    "3. **Training:** Fit the model, monitoring loss and accuracy.\n",
    "4. **Evaluation:** Compute test loss, accuracy, and perplexity.\n",
    "5. **Generation:** Use the trained model to produce a sample, with temperature for variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532fd627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.1\n",
      "Built with CUDA: True\n",
      "Num GPUs Available: 1\n",
      "Device list: [LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Device list:\", tf.config.list_logical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19055958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Shakespeare text...\n",
      "Downloaded 1,115,394 characters\n",
      "Cleaned length: 1,108,157 characters\n",
      "Vocabulary size: 64\n",
      "First 80 chars: First Citizen: Before we proceed any further, hear me speak. All: Speak, speak. \n",
      "Sample chars:  !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZab ...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Load and prepare the dataset\n",
    "def load_shakespeare_text():\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    print(\"Downloading Shakespeare text...\")\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to download text\")\n",
    "    text = response.text\n",
    "    print(f\"Downloaded {len(text):,} characters\")\n",
    "    return text\n",
    "\n",
    "text = load_shakespeare_text()\n",
    "\n",
    "# Clean\n",
    "text = text.replace('\\n', ' ').replace('  ', ' ').strip()\n",
    "print(f\"Cleaned length: {len(text):,} characters\")\n",
    "\n",
    "# Vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "n_vocab = len(chars)\n",
    "char2idx = {c: i for i, c in enumerate(chars)}\n",
    "idx2char = {i: c for c, i in char2idx.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {n_vocab}\")\n",
    "print(\"First 80 chars:\", text[:80])\n",
    "print(\"Sample chars:\", ''.join(chars[:40]), \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b11a4bb",
   "metadata": {},
   "source": [
    "#### Prepare Data Sequences and Train/Test Split\n",
    "\n",
    "Convert the text to integer indices, then create input sequences (X) of length 100 and corresponding targets (y) as the next character. This frames the problem as supervised learning: given 100 chars, predict the 101st. Use a step size of 3 to reduce overlap and computation while retaining diversity. Split into train (90%) and test (10%) sets for evaluation. This step uses NumPy for array manipulation. Source: Sequence creation mirrors the TensorFlow tutorial's dataset pipeline, adapted to NumPy for simplicity; train/test split follows standard ML practices from Keras documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e9d3b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 332417\n",
      "Test samples: 36936\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "seq_length = 100  # Sequence length for input\n",
    "step = 3  # Step size to slide window over text\n",
    "\n",
    "# Convert text to integers\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "# Create inputs and targets\n",
    "inputs = []\n",
    "targets = []\n",
    "for i in range(0, len(text_as_int) - seq_length, step):\n",
    "    inputs.append(text_as_int[i:i + seq_length])\n",
    "    targets.append(text_as_int[i + seq_length])\n",
    "\n",
    "X = np.array(inputs)\n",
    "y = to_categorical(targets, num_classes=n_vocab)  # One-hot encode targets\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f098fbd5",
   "metadata": {},
   "source": [
    "#### Build the LSTM Model\n",
    "\n",
    "**Define a sequential Keras model:** Embedding turns integer inputs into dense vectors (256 dims), two LSTM layers capture sequential dependencies (512 units each, with dropout for regularization), and a dense layer outputs probabilities over the vocabulary. The first LSTM returns sequences to feed into the second. Compile with categorical cross-entropy (suitable for multi-class prediction) and Adam optimizer. Source: Model architecture adapted from the Medium guide on Shakespeare LSTM generator (uses stacked LSTMs with dropout); embedding and LSTM params align with TensorFlow's RNN tutorial recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6efb2cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 256)          16384     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100, 512)          1574912   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 512)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 512)               2099200   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                32832     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3723328 (14.20 MB)\n",
      "Trainable params: 3723328 (14.20 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab, 256, input_length=seq_length))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_vocab, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac45c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-21 01:22:24.982434: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 265933600 exceeds 10% of free system memory.\n",
      "2026-02-21 01:22:25.311834: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 265933600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-21 01:22:27.608161: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2026-02-21 01:22:28.333736: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f5c544b5fc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2026-02-21 01:22:28.333774: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1050 Ti, Compute Capability 6.1\n",
      "2026-02-21 01:22:28.339881: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1771654948.430869 1265831 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5195/5195 [==============================] - 694s 133ms/step - loss: 1.9064 - accuracy: 0.4507 - val_loss: 1.5800 - val_accuracy: 0.5314\n",
      "Epoch 2/20\n",
      "5195/5195 [==============================] - 695s 134ms/step - loss: 1.5201 - accuracy: 0.5460 - val_loss: 1.4757 - val_accuracy: 0.5567\n",
      "Epoch 3/20\n",
      "5195/5195 [==============================] - 691s 133ms/step - loss: 1.4233 - accuracy: 0.5696 - val_loss: 1.4388 - val_accuracy: 0.5703\n",
      "Epoch 4/20\n",
      "5195/5195 [==============================] - 692s 133ms/step - loss: 1.3690 - accuracy: 0.5826 - val_loss: 1.4177 - val_accuracy: 0.5762\n",
      "Epoch 5/20\n",
      "5195/5195 [==============================] - 687s 132ms/step - loss: 1.3295 - accuracy: 0.5911 - val_loss: 1.4104 - val_accuracy: 0.5792\n",
      "Epoch 6/20\n",
      "5195/5195 [==============================] - 667s 128ms/step - loss: 1.2983 - accuracy: 0.5995 - val_loss: 1.4095 - val_accuracy: 0.5777\n",
      "Epoch 7/20\n",
      "5195/5195 [==============================] - 693s 133ms/step - loss: 1.2763 - accuracy: 0.6053 - val_loss: 1.4135 - val_accuracy: 0.5795\n",
      "Epoch 8/20\n",
      "5195/5195 [==============================] - 685s 132ms/step - loss: 1.2607 - accuracy: 0.6080 - val_loss: 1.4109 - val_accuracy: 0.5797\n",
      "Epoch 9/20\n",
      "5195/5195 [==============================] - 689s 133ms/step - loss: 1.2460 - accuracy: 0.6116 - val_loss: 1.4237 - val_accuracy: 0.5776\n",
      "Epoch 10/20\n",
      "5195/5195 [==============================] - 682s 131ms/step - loss: 1.2377 - accuracy: 0.6133 - val_loss: 1.4230 - val_accuracy: 0.5812\n",
      "Epoch 11/20\n",
      "5195/5195 [==============================] - 684s 132ms/step - loss: 1.2338 - accuracy: 0.6141 - val_loss: 1.4263 - val_accuracy: 0.5789\n",
      "Epoch 12/20\n",
      "5195/5195 [==============================] - 687s 132ms/step - loss: 1.2290 - accuracy: 0.6151 - val_loss: 1.4252 - val_accuracy: 0.5790\n",
      "Epoch 13/20\n",
      "5195/5195 [==============================] - 690s 133ms/step - loss: 1.2292 - accuracy: 0.6150 - val_loss: 1.4303 - val_accuracy: 0.5788\n",
      "Epoch 14/20\n",
      "5195/5195 [==============================] - 688s 132ms/step - loss: 1.2297 - accuracy: 0.6148 - val_loss: 1.4282 - val_accuracy: 0.5780\n",
      "Epoch 15/20\n",
      "5195/5195 [==============================] - 686s 132ms/step - loss: 1.2312 - accuracy: 0.6135 - val_loss: 1.4318 - val_accuracy: 0.5790\n",
      "Epoch 16/20\n",
      "5195/5195 [==============================] - 691s 133ms/step - loss: 1.2362 - accuracy: 0.6131 - val_loss: 1.4328 - val_accuracy: 0.5758\n",
      "Epoch 17/20\n",
      "5195/5195 [==============================] - 681s 131ms/step - loss: 1.2413 - accuracy: 0.6105 - val_loss: 1.4281 - val_accuracy: 0.5780\n",
      "Epoch 18/20\n",
      "5195/5195 [==============================] - 685s 132ms/step - loss: 1.2439 - accuracy: 0.6109 - val_loss: 1.4422 - val_accuracy: 0.5765\n",
      "Epoch 19/20\n",
      "5195/5195 [==============================] - 687s 132ms/step - loss: 1.2506 - accuracy: 0.6088 - val_loss: 1.4381 - val_accuracy: 0.5741\n",
      "Epoch 20/20\n",
      "5195/5195 [==============================] - 685s 132ms/step - loss: 1.2551 - accuracy: 0.6073 - val_loss: 1.4326 - val_accuracy: 0.5768\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd8ba73",
   "metadata": {},
   "source": [
    "#### Evaluate Metrics\n",
    "\n",
    "Compute test loss and accuracy, then derive perplexity. Lower perplexity means better uncertainty handling. Typical values: If test loss is 1.6, perplexity ~5 indicates the model is about 5x as uncertain as a perfect model. Source: Perplexity calculation from Machine Learning Mastery LSTM text gen post; evaluation aligns with Keras metrics in the sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae048a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1155/1155 [==============================] - 37s 32ms/step - loss: 1.4326 - accuracy: 0.5768\n",
      "Test Loss: 1.4326\n",
      "Test Accuracy: 0.5768\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "perplexity = np.exp(test_loss)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa206da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1265777/2293857167.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  preds = np.log(preds) / temperature\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poetry Sample:\n",
      " Shall I compare thee to a summer's day biuvIbisvlepliubbalaaauwRwlAhdI-vlIauspIjJK-ATKiFftnnlAyaOLOG ARIEL: Good heart-place with man at a king when he has not for love. SICINIUS: No, I say; that thou respects with the king's life of this way The time for the great boar when they be mountly weeping her man. Gremio, the commons the county; And he is forget so far and triumphant on his trime, Stands yet forgive the hands. Give me to make my way of my incrain to have the people. BAPTISTA: But these are regains; methinks it not the time \n"
     ]
    }
   ],
   "source": [
    "# Generate Poetry Sample\n",
    "def generate_text(model, char2idx, idx2char, seed_text, n_chars=500, temperature=0.7):\n",
    "    \"\"\" Define a function to generate text: Start with a seed, predict next characters autoregressively using temperature-scaled sampling \n",
    "    (lower temperature = more predictable; higher = more creative). Generate 500 characters as a \"poetry sample\" (prose-like due to cleaning). \n",
    "    Run with a Shakespearean seed.\"\"\"\n",
    "    \n",
    "    generated = seed_text\n",
    "    for _ in range(n_chars):\n",
    "        x_pred = np.zeros((1, seq_length))\n",
    "        seq = generated[-seq_length:]  # Take last seq_length chars\n",
    "        for t, char in enumerate(seq):\n",
    "            if char in char2idx:  # Handle if seed has unseen chars\n",
    "                x_pred[0, t] = char2idx[char]\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        next_index = np.random.choice(len(preds), p=preds)\n",
    "        next_char = idx2char[next_index]\n",
    "        generated += next_char\n",
    "    return generated\n",
    "\n",
    "# Example usage\n",
    "seed = \"Shall I compare thee to a summer's day \"\n",
    "sample = generate_text(model, char2idx, idx2char, seed, n_chars=500, temperature=0.7)\n",
    "print(\"Generated Poetry Sample:\\n\", sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WLV-TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
