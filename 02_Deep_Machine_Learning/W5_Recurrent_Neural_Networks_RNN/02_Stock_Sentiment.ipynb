{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "081b1cd9",
   "metadata": {},
   "source": [
    "### 5.5: Stock Sentiment Analysis using RNN\n",
    "\n",
    "##### Objective\n",
    "\n",
    "This workshop aims to explore Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) architectures for text-based tasks. We will learn how to preprocess text data (through tokenisation and word embedding), apply RNNs to a news classification problem, and use LSTMs for text generation tasks such as poetry creation.\n",
    "\n",
    "##### Learning Outcomes\n",
    "\n",
    "1. **Text Tokenization and Embedding:** Learn how to transform raw text into a suitable input format (tokens) and represent words or tokens as dense vector embeddings for neural network processing.\n",
    "2. **RNN-Based Classification:** The AG News dataset exemplifies applying a simple RNN model to a text classification task.\n",
    "3. **LSTM for Text Generation:** Gain hands-on experience building an LSTM model to generate text using the Adele.txt poetry dataset as a case study.\n",
    "\n",
    "##### Introduction to Text Modelling for Neural Networks\n",
    "\n",
    "When dealing with textual data, we must convert language into numerical representations that neural networks can process. This involves two key steps:\n",
    "\n",
    "1. **Tokenisation:** Splitting the text into smaller units, usually words (or sometimes sub-words or characters).\n",
    "2. **Word Embedding:** Mapping each token to a continuous vector space (e.g., via word2vec or Glove-like embeddings) so semantically similar tokens have similar vector representations.\n",
    "\n",
    "These representations capture syntactic and semantic relationships, enabling models to learn patterns across sequences of words.\n",
    "\n",
    "\n",
    "##### Recurrent Neural Networks (RNN) and LSTM\n",
    "\n",
    "`Recurrent Neural Networks (RNNs)` are designed to handle sequential data by maintaining a hidden state that propagates information from one time step to the next. Traditional RNNs, however, often struggle with long-term dependencies due to vanishing or exploding gradients.\n",
    "\n",
    "`Long-short-term memory (LSTM)` networks address these issues by introducing a memory cell and gating mechanisms (input, output, and forget gates) that regulate how information flows through the network. This allows the model to maintain longer-range dependencies, making it especially effective for language modelling and text generation tasks.\n",
    "\n",
    "\n",
    "##### Example Applications\n",
    "\n",
    "1. **AG News Classification:**\n",
    "    - Use tokenized AG News articles and feed them into an RNN architecture for topic classification.\n",
    "    - The model learns to identify news categories (e.g., World, Business, Sports) by capturing patterns in word usage.\n",
    "2. **LSTM for Poetry Generation:**\n",
    "    - Train an LSTM on the Adele.txt dataset (or any poetry corpus) to learn linguistic styles and generate new verses.\n",
    "    - By sampling from the trained model, you can produce novel lines of text that mimic the style of the training data.\n",
    "\n",
    "##### Conclusion\n",
    "\n",
    "In this workshop, you explored how tokenisation, word embedding, and recurrent architectures (RNN, LSTM) form the backbone of many Natural Language Processing (NLP) applications. By implementing classification and text-generation tasks, you gain practical insights into how neural networks handle sequential data. These skills will be a solid foundation for more advanced NLP techniques, such as Transformer-based models and attention mechanisms.     \n",
    "\n",
    "\n",
    "##### **Sources**\n",
    "- https://newsapi.org/\n",
    "- https://newsapi.org/docs/endpoints/everything\n",
    "- https://www.researchgate.net/publication/363860201_Sentimental_Classification_of_News_Headlines_using_Recurrent_Neural_Network\n",
    "- https://www.ijert.org/text-classification-using-rnn\n",
    "- https://www.geeksforgeeks.org/nlp/rnn-for-text-classifications-in-nlp/\n",
    "- https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "- https://dev.to/aionlinecourse/learn-how-to-build-multi-class-text-classification-models-with-rnn-and-lstm-ned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b34b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split        \n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac685691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 Important Headlines:\n",
      "===================================\n",
      "1. The Shocking Reason This Analyst Says Michael Saylor and MicroStrategy Stock Will Take Bitcoin Prices to $0\n",
      "   Source: Barchart.com\n",
      "   Link: https://www.barchart.com/story/news/113796/the-shocking-reason-this-analyst-says-michael-saylor-and-microstrategy-stock-will-take-bitcoin-prices-to-0\n",
      "\n",
      "2. 'If People in the Rest of the World Knew What I Know': MicroStrategy's Michael Saylor's Viral Message About MSTR Stock and Bitcoin to $10 Million\n",
      "   Source: Yahoo Entertainment\n",
      "   Link: https://consent.yahoo.com/v2/collectConsent?sessionId=1_cc-session_8062cab1-ad18-4f12-9a69-47d8ac81e40d\n",
      "\n",
      "3. Why MicroStrategy’s Latest Bitcoin Purchase Is Deeply Concerning\n",
      "   Source: BeInCrypto\n",
      "   Link: https://beincrypto.com/microstrategy-latest-bitcoin-buy-4-reasons-concerning/\n",
      "\n",
      "4. Stock market today: Dow soars 1,000 points, leading S&P 500, Nasdaq higher as Wall Street rebounds from rout\n",
      "   Source: Yahoo Entertainment\n",
      "   Link: https://finance.yahoo.com/news/live/stock-market-today-dow-crosses-50000-mark-leading-sp-500-nasdaq-higher-as-wall-street-rebounds-from-rout-192439513.html\n",
      "\n",
      "5. Bitcoin/Crypto Crash Captures Headlines as Potentially More Serious Tech-Driven Debt Retreat Progresses\n",
      "   Source: Nakedcapitalism.com\n",
      "   Link: https://www.nakedcapitalism.com/2026/02/bitcoin-crypto-crash-captures-headlines-as-potentially-more-serious-tech-driven-debt-retreat-progresses.html\n",
      "\n",
      "6. ETF that feasts on carnage in bitcoin-holder Strategy hits record high\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/02/04/etf-that-feasts-on-carnage-in-bitcoin-holder-strategy-hits-record-high\n",
      "\n",
      "7. Michael Saylor's Strategy purchased $168 million in bitcoin last week\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/02/17/michael-saylor-s-strategy-purchased-usd168-million-in-bitcoin-last-week\n",
      "\n",
      "8. Strategy purchased $264 million in bitcoin last week, a slowdown from recent acquisition pace\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/01/26/strategy-purchased-usd264-million-in-bitcoin-last-week\n",
      "\n",
      "9. Michael Saylor's Strategy made modest bitcoin purchase at start of last week's crypto crash\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/02/09/michael-saylor-s-strategy-made-modest-bitcoin-acquisition-during-last-week-s-crypto-crash\n",
      "\n",
      "10. Strategy's STRC returns to $100, poised to unlock more bitcoin accumulation\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/02/12/strategy-s-strc-returns-to-usd100-poised-to-unlock-more-bitcoin-accumulation\n",
      "\n",
      "11. Strategy to initiate a bitcoin security program addressing quantum uncertainty\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/02/06/strategy-to-initiate-a-bitcoin-security-program-addressing-quantum-uncertainty\n",
      "\n",
      "12. Strategy slides toward eighth straight monthly decline\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/02/03/strategy-slides-toward-eighth-straight-monthly-decline\n",
      "\n",
      "13. Michael Saylor's Strategy added $75 million in bitcoin to holdings prior to last week's crash\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/02/02/michael-saylor-s-strategy-added-usd75-million-in-bitcoin-to-holdings-prior-to-last-week-s-crash\n",
      "\n",
      "14. Strategy has $6.5 billion loss on BTC, but continues trading at premium to value of its assets\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/02/05/strategy-has-usd6-5-billion-loss-on-btc-but-continues-trading-at-premium-to-value-of-its-assets\n",
      "\n",
      "15. Harvard cuts bitcoin exposure by 20%, adds new ether position\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/business/2026/02/16/harvard-cuts-bitcoin-exposure-by-20-adds-new-ether-position\n",
      "\n"
     ]
    }
   ],
   "source": [
    "API_KEY = 'c2a7637911fd4e9ba350e58aa16f299e'\n",
    "query = 'MSTR'\n",
    "\n",
    "# Define the endpoint and parameters\n",
    "url = f'https://newsapi.org/v2/everything'\n",
    "params = {\n",
    "    'q': query,\n",
    "    'sortBy': 'popularity',\n",
    "    'language': 'en',\n",
    "    'pageSize': 15,  # Limits the results to 15 articles\n",
    "    'apiKey': API_KEY\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Execute the request\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    # Print the list of 15 headlines\n",
    "    articles = data.get('articles', [])\n",
    "    \n",
    "    print(f\"Top 15 Important Headlines:\\n\" + \"=\"*35)\n",
    "    for i, article in enumerate(articles, 1):\n",
    "        print(f\"{i}. {article['title']}\")\n",
    "        print(f\"   Source: {article['source']['name']}\")\n",
    "        print(f\"   Link: {article['url']}\\n\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching news: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb644772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The Shocking Reason This Analyst Says Michael Saylor and MicroStrategy Stock Will Take Bitcoin Prices to $0\n",
      "2. 'If People in the Rest of the World Knew What I Know': MicroStrategy's Michael Saylor's Viral Message About MSTR Stock and Bitcoin to $10 Million\n",
      "3. Why MicroStrategy’s Latest Bitcoin Purchase Is Deeply Concerning\n",
      "4. Stock market today: Dow soars 1,000 points, leading S&P 500, Nasdaq higher as Wall Street rebounds from rout\n",
      "5. Bitcoin/Crypto Crash Captures Headlines as Potentially More Serious Tech-Driven Debt Retreat Progresses\n",
      "6. ETF that feasts on carnage in bitcoin-holder Strategy hits record high\n",
      "7. Michael Saylor's Strategy purchased $168 million in bitcoin last week\n",
      "8. Strategy purchased $264 million in bitcoin last week, a slowdown from recent acquisition pace\n",
      "9. Michael Saylor's Strategy made modest bitcoin purchase at start of last week's crypto crash\n",
      "10. Strategy's STRC returns to $100, poised to unlock more bitcoin accumulation\n",
      "11. Strategy to initiate a bitcoin security program addressing quantum uncertainty\n",
      "12. Strategy slides toward eighth straight monthly decline\n",
      "13. Michael Saylor's Strategy added $75 million in bitcoin to holdings prior to last week's crash\n",
      "14. Strategy has $6.5 billion loss on BTC, but continues trading at premium to value of its assets\n",
      "15. Harvard cuts bitcoin exposure by 20%, adds new ether position\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    headlines = [article['title'] for article in data.get('articles', [])]\n",
    "    for i, headline in enumerate(headlines, 1):\n",
    "        print(f\"{i}. {headline}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa968af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 145\n",
      "Max sequence length: 25\n",
      "Padded shape: (15, 25)\n"
     ]
    }
   ],
   "source": [
    "# Manual sentiment labels (0=positive, 1=negative, 2=neutral)\n",
    "labels = [1, 1, 2, 0, 2, 1, 1, 0, 0, 2, 0, 0, 1, 0, 2]\n",
    "\n",
    "# Convert labels to categorical\n",
    "labels_cat = tf.keras.utils.to_categorical(labels, num_classes=3)\n",
    "\n",
    "# Preprocessing: Tokenization and Padding\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(headlines)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(headlines)\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Max sequence length: {max_len}\")\n",
    "print(f\"Padded shape: {padded_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e9615e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (12, 25), Test shape: (3, 25)\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_sequences,\n",
    "    labels_cat,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels                \n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a3569b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 50)            7250      \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 128)               22912     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                4128      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34389 (134.33 KB)\n",
      "Trainable params: 34389 (134.33 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 23:12:42.789364: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-19 23:12:42.833290: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-19 23:12:42.833571: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-19 23:12:42.834802: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-19 23:12:42.835065: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-19 23:12:42.835371: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-19 23:12:42.907147: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-19 23:12:42.907357: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-19 23:12:42.907516: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-19 23:12:42.907629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3474 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Build and compile RNN model\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                              output_dim=50,\n",
    "                              input_length=max_len),\n",
    "    tf.keras.layers.SimpleRNN(128, return_sequences=False),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddfc4346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 23:13:10.202376: I external/local_xla/xla/service/service.cc:168] XLA service 0x72804046a470 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2026-02-19 23:13:10.202404: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1050 Ti, Compute Capability 6.1\n",
      "2026-02-19 23:13:10.212062: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2026-02-19 23:13:10.230284: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1771560790.311476 1164945 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 3s 240ms/step - loss: 1.0952 - accuracy: 0.3333 - val_loss: 1.0868 - val_accuracy: 0.3333\n",
      "Epoch 2/25\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.8624 - accuracy: 0.8889 - val_loss: 1.0780 - val_accuracy: 0.3333\n",
      "Epoch 3/25\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 0.7640 - accuracy: 0.6667 - val_loss: 1.1366 - val_accuracy: 0.3333\n",
      "Epoch 4/25\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.6537 - accuracy: 0.6667 - val_loss: 1.0432 - val_accuracy: 0.3333\n",
      "Epoch 5/25\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.5228 - accuracy: 0.7778 - val_loss: 1.0254 - val_accuracy: 0.6667\n",
      "Epoch 6/25\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.3700 - accuracy: 1.0000 - val_loss: 0.9856 - val_accuracy: 0.6667\n",
      "Epoch 7/25\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.3169 - accuracy: 1.0000 - val_loss: 0.9742 - val_accuracy: 0.6667\n",
      "Epoch 8/25\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.1761 - accuracy: 1.0000 - val_loss: 1.0312 - val_accuracy: 0.6667\n",
      "Epoch 9/25\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.2377 - accuracy: 0.8889 - val_loss: 1.0261 - val_accuracy: 0.6667\n",
      "Epoch 10/25\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.1874 - accuracy: 1.0000 - val_loss: 0.9676 - val_accuracy: 0.6667\n",
      "Epoch 11/25\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.1402 - accuracy: 1.0000 - val_loss: 0.9749 - val_accuracy: 0.6667\n",
      "Epoch 12/25\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 1.2020 - val_accuracy: 0.3333\n",
      "Epoch 13/25\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 1.4546 - val_accuracy: 0.3333\n",
      "Epoch 14/25\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 1.6256 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/25\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 0.0596 - accuracy: 1.0000 - val_loss: 1.5718 - val_accuracy: 0.3333\n",
      "Epoch 16/25\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 0.0382 - accuracy: 1.0000 - val_loss: 1.4643 - val_accuracy: 0.3333\n",
      "Epoch 17/25\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.1043 - accuracy: 1.0000 - val_loss: 1.2839 - val_accuracy: 0.3333\n",
      "Epoch 18/25\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.9123 - val_accuracy: 0.3333\n",
      "Epoch 19/25\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.0663 - accuracy: 1.0000 - val_loss: 0.6744 - val_accuracy: 0.6667\n",
      "Epoch 20/25\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.5618 - val_accuracy: 0.6667\n",
      "Epoch 21/25\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.5061 - val_accuracy: 0.6667\n",
      "Epoch 22/25\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.0371 - accuracy: 1.0000 - val_loss: 0.5156 - val_accuracy: 0.6667\n",
      "Epoch 23/25\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.5897 - val_accuracy: 0.6667\n",
      "Epoch 24/25\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.0230 - accuracy: 1.0000 - val_loss: 0.7010 - val_accuracy: 0.6667\n",
      "Epoch 25/25\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.8064 - val_accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=25,\n",
    "    batch_size=4,\n",
    "    validation_split=0.25,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f872f9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss:     0.9013\n",
      "Test Accuracy: 0.6667\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive      1.000     1.000     1.000         1\n",
      "    Negative      0.500     1.000     0.667         1\n",
      "     Neutral      0.000     0.000     0.000         1\n",
      "\n",
      "    accuracy                          0.667         3\n",
      "   macro avg      0.500     0.667     0.556         3\n",
      "weighted avg      0.500     0.667     0.556         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set + detailed metrics\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Loss:     {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_prob = model.predict(X_test, verbose=0)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Full classification report\n",
    "target_names = ['Positive', 'Negative', 'Neutral']\n",
    "cr = classification_report(y_true, y_pred, target_names=target_names, digits=3, zero_division=0)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ddf334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: MSTR Has Lost 62 percent in a Year and Bitcoin Is Still Below Its Buy Price\n",
      "Probabilities: Positive 0.170 | Negative 0.769 | Neutral 0.061\n",
      "Predicted sentiment: Negative (class 1)\n",
      "Positive score: 0.170\n"
     ]
    }
   ],
   "source": [
    "new_headline = \"MSTR Has Lost 62 percent in a Year and Bitcoin Is Still Below Its Buy Price\"\n",
    "\n",
    "# Preprocess\n",
    "seq = tokenizer.texts_to_sequences([new_headline])\n",
    "padded = pad_sequences(seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Predict\n",
    "probs = model.predict(padded, verbose=0)[0]\n",
    "pred_class = np.argmax(probs)\n",
    "\n",
    "labels_map = {0: \"Positive\", 1: \"Negative\", 2: \"Neutral\"}\n",
    "\n",
    "print(f\"Headline: {new_headline}\")\n",
    "print(f\"Probabilities: Positive {probs[0]:.3f} | Negative {probs[1]:.3f} | Neutral {probs[2]:.3f}\")\n",
    "print(f\"Predicted sentiment: {labels_map[pred_class]} (class {pred_class})\")\n",
    "print(f\"Positive score: {probs[0]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WLV-AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
