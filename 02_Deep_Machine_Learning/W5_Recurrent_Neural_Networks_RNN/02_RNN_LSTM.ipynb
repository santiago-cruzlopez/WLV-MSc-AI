{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "081b1cd9",
   "metadata": {},
   "source": [
    "### 5.7: News Classifications\n",
    "\n",
    "##### Objective\n",
    "\n",
    "This workshop aims to explore Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) architectures for text-based tasks. We will learn how to preprocess text data (through tokenisation and word embedding), apply RNNs to a news classification problem, and use LSTMs for text generation tasks such as poetry creation.\n",
    "\n",
    "##### Learning Outcomes\n",
    "\n",
    "1. **Text Tokenization and Embedding:** Learn how to transform raw text into a suitable input format (tokens) and represent words or tokens as dense vector embeddings for neural network processing.\n",
    "2. **RNN-Based Classification:** The AG News dataset exemplifies applying a simple RNN model to a text classification task.\n",
    "3. **LSTM for Text Generation:** Gain hands-on experience building an LSTM model to generate text using the Adele.txt poetry dataset as a case study.\n",
    "\n",
    "##### Introduction to Text Modelling for Neural Networks\n",
    "\n",
    "When dealing with textual data, we must convert language into numerical representations that neural networks can process. This involves two key steps:\n",
    "\n",
    "1. **Tokenisation:** Splitting the text into smaller units, usually words (or sometimes sub-words or characters).\n",
    "2. **Word Embedding:** Mapping each token to a continuous vector space (e.g., via word2vec or Glove-like embeddings) so semantically similar tokens have similar vector representations.\n",
    "\n",
    "These representations capture syntactic and semantic relationships, enabling models to learn patterns across sequences of words.\n",
    "\n",
    "\n",
    "##### Recurrent Neural Networks (RNN) and LSTM\n",
    "\n",
    "`Recurrent Neural Networks (RNNs)` are designed to handle sequential data by maintaining a hidden state that propagates information from one time step to the next. Traditional RNNs, however, often struggle with long-term dependencies due to vanishing or exploding gradients.\n",
    "\n",
    "`Long-short-term memory (LSTM)` networks address these issues by introducing a memory cell and gating mechanisms (input, output, and forget gates) that regulate how information flows through the network. This allows the model to maintain longer-range dependencies, making it especially effective for language modelling and text generation tasks.\n",
    "\n",
    "\n",
    "##### Example Applications\n",
    "\n",
    "1. **AG News Classification:**\n",
    "    - Use tokenized AG News articles and feed them into an RNN architecture for topic classification.\n",
    "    - The model learns to identify news categories (e.g., World, Business, Sports) by capturing patterns in word usage.\n",
    "2. **LSTM for Poetry Generation:**\n",
    "    - Train an LSTM on the Adele.txt dataset (or any poetry corpus) to learn linguistic styles and generate new verses.\n",
    "    - By sampling from the trained model, you can produce novel lines of text that mimic the style of the training data.\n",
    "\n",
    "##### Student Task\n",
    "\n",
    "- Select a book dataset of your choice and preprocess the text for training.\n",
    "- Implement a bidirectional LSTM or GRU model using TensorFlow for text generation.\n",
    "- Train the model to learn the text style and structure, then generate new text samples.\n",
    "- Share your code, model configuration, training strategies, and generated samples in the collaborative coding discussion forum.\n",
    "\n",
    "##### Conclusion\n",
    "\n",
    "In this workshop, you explored how tokenisation, word embedding, and recurrent architectures (RNN, LSTM) form the backbone of many Natural Language Processing (NLP) applications. By implementing classification and text-generation tasks, you gain practical insights into how neural networks handle sequential data. These skills will be a solid foundation for more advanced NLP techniques, such as Transformer-based models and attention mechanisms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac685691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 Important Headlines:\n",
      "===================================\n",
      "1. The Shocking Reason This Analyst Says Michael Saylor and MicroStrategy Stock Will Take Bitcoin Prices to $0\n",
      "   Source: Barchart.com\n",
      "   Link: https://www.barchart.com/story/news/113796/the-shocking-reason-this-analyst-says-michael-saylor-and-microstrategy-stock-will-take-bitcoin-prices-to-0\n",
      "\n",
      "2. 'If People in the Rest of the World Knew What I Know': MicroStrategy's Michael Saylor's Viral Message About MSTR Stock and Bitcoin to $10 Million\n",
      "   Source: Yahoo Entertainment\n",
      "   Link: https://consent.yahoo.com/v2/collectConsent?sessionId=1_cc-session_8062cab1-ad18-4f12-9a69-47d8ac81e40d\n",
      "\n",
      "3. Why MicroStrategy’s Latest Bitcoin Purchase Is Deeply Concerning\n",
      "   Source: BeInCrypto\n",
      "   Link: https://beincrypto.com/microstrategy-latest-bitcoin-buy-4-reasons-concerning/\n",
      "\n",
      "4. Stock market today: Dow soars 1,000 points, leading S&P 500, Nasdaq higher as Wall Street rebounds from rout\n",
      "   Source: Yahoo Entertainment\n",
      "   Link: https://finance.yahoo.com/news/live/stock-market-today-dow-crosses-50000-mark-leading-sp-500-nasdaq-higher-as-wall-street-rebounds-from-rout-192439513.html\n",
      "\n",
      "5. Bitcoin/Crypto Crash Captures Headlines as Potentially More Serious Tech-Driven Debt Retreat Progresses\n",
      "   Source: Nakedcapitalism.com\n",
      "   Link: https://www.nakedcapitalism.com/2026/02/bitcoin-crypto-crash-captures-headlines-as-potentially-more-serious-tech-driven-debt-retreat-progresses.html\n",
      "\n",
      "6. ETF that feasts on carnage in bitcoin-holder Strategy hits record high\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/02/04/etf-that-feasts-on-carnage-in-bitcoin-holder-strategy-hits-record-high\n",
      "\n",
      "7. Michael Saylor's Strategy purchased $168 million in bitcoin last week\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/02/17/michael-saylor-s-strategy-purchased-usd168-million-in-bitcoin-last-week\n",
      "\n",
      "8. Strategy purchased $264 million in bitcoin last week, a slowdown from recent acquisition pace\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/01/26/strategy-purchased-usd264-million-in-bitcoin-last-week\n",
      "\n",
      "9. Michael Saylor's Strategy made modest bitcoin purchase at start of last week's crypto crash\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/02/09/michael-saylor-s-strategy-made-modest-bitcoin-acquisition-during-last-week-s-crypto-crash\n",
      "\n",
      "10. Strategy's STRC returns to $100, poised to unlock more bitcoin accumulation\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/02/12/strategy-s-strc-returns-to-usd100-poised-to-unlock-more-bitcoin-accumulation\n",
      "\n",
      "11. Strategy to initiate a bitcoin security program addressing quantum uncertainty\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/02/06/strategy-to-initiate-a-bitcoin-security-program-addressing-quantum-uncertainty\n",
      "\n",
      "12. Strategy slides toward eighth straight monthly decline\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/02/03/strategy-slides-toward-eighth-straight-monthly-decline\n",
      "\n",
      "13. Michael Saylor's Strategy added $75 million in bitcoin to holdings prior to last week's crash\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/02/02/michael-saylor-s-strategy-added-usd75-million-in-bitcoin-to-holdings-prior-to-last-week-s-crash\n",
      "\n",
      "14. Strategy has $6.5 billion loss on BTC, but continues trading at premium to value of its assets\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/markets/2026/02/05/strategy-has-usd6-5-billion-loss-on-btc-but-continues-trading-at-premium-to-value-of-its-assets\n",
      "\n",
      "15. Harvard cuts bitcoin exposure by 20%, adds new ether position\n",
      "   Source: CoinDesk\n",
      "   Link: https://www.coindesk.com/business/2026/02/16/harvard-cuts-bitcoin-exposure-by-20-adds-new-ether-position\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_KEY = 'c2a7637911fd4e9ba350e58aa16f299e'\n",
    "query = 'MSTR'\n",
    "\n",
    "# Define the endpoint and parameters\n",
    "url = f'https://newsapi.org/v2/everything'\n",
    "params = {\n",
    "    'q': query,\n",
    "    'sortBy': 'popularity',\n",
    "    'language': 'en',\n",
    "    'pageSize': 15,  # Limits the results to 15 articles\n",
    "    'apiKey': API_KEY\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Execute the request\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    # Print the list of 15 headlines\n",
    "    articles = data.get('articles', [])\n",
    "    \n",
    "    print(f\"Top 15 Important Headlines:\\n\" + \"=\"*35)\n",
    "    for i, article in enumerate(articles, 1):\n",
    "        print(f\"{i}. {article['title']}\")\n",
    "        print(f\"   Source: {article['source']['name']}\")\n",
    "        print(f\"   Link: {article['url']}\\n\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching news: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb644772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The Shocking Reason This Analyst Says Michael Saylor and MicroStrategy Stock Will Take Bitcoin Prices to $0\n",
      "2. 'If People in the Rest of the World Knew What I Know': MicroStrategy's Michael Saylor's Viral Message About MSTR Stock and Bitcoin to $10 Million\n",
      "3. Why MicroStrategy’s Latest Bitcoin Purchase Is Deeply Concerning\n",
      "4. Stock market today: Dow soars 1,000 points, leading S&P 500, Nasdaq higher as Wall Street rebounds from rout\n",
      "5. Bitcoin/Crypto Crash Captures Headlines as Potentially More Serious Tech-Driven Debt Retreat Progresses\n",
      "6. ETF that feasts on carnage in bitcoin-holder Strategy hits record high\n",
      "7. Michael Saylor's Strategy purchased $168 million in bitcoin last week\n",
      "8. Strategy purchased $264 million in bitcoin last week, a slowdown from recent acquisition pace\n",
      "9. Strategy's STRC returns to $100, poised to unlock more bitcoin accumulation\n",
      "10. Michael Saylor's Strategy made modest bitcoin purchase at start of last week's crypto crash\n",
      "11. Strategy to initiate a bitcoin security program addressing quantum uncertainty\n",
      "12. Strategy slides toward eighth straight monthly decline\n",
      "13. Michael Saylor's Strategy added $75 million in bitcoin to holdings prior to last week's crash\n",
      "14. Strategy has $6.5 billion loss on BTC, but continues trading at premium to value of its assets\n",
      "15. Harvard cuts bitcoin exposure by 20%, adds new ether position\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    headlines = [article['title'] for article in data.get('articles', [])]\n",
    "    for i, headline in enumerate(headlines, 1):\n",
    "        print(f\"{i}. {headline}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Sample labels (0: positive, 1: negative, 2: neutral)\n",
    "labels = [1, 1, 1, 0, 2, 1, 1, 0, 0, 2, 0, 0, 1, 0, 2]  # Corresponding to headlines\n",
    "\n",
    "# Preprocessing: Tokenization and Padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(headlines)\n",
    "sequences = tokenizer.texts_to_sequences(headlines)\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert labels to categorical\n",
    "labels_cat = tf.keras.utils.to_categorical(labels, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a3569b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 25, 50)            7200      \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 128)               22912     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30499 (119.14 KB)\n",
      "Trainable params: 30499 (119.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build Simple RNN Model (can replace RNN with LSTM for better performance)\n",
    "model_class = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 50, input_length=max_len),\n",
    "    tf.keras.layers.SimpleRNN(128),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model_class.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_class.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddfc4346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 220ms/step - loss: 1.1259 - accuracy: 0.3333 - val_loss: 1.1545 - val_accuracy: 0.3333\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 0.7386 - accuracy: 0.9167 - val_loss: 1.1422 - val_accuracy: 0.3333\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 0.4123 - accuracy: 1.0000 - val_loss: 1.0898 - val_accuracy: 0.6667\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.1331 - accuracy: 1.0000 - val_loss: 0.9903 - val_accuracy: 0.6667\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.0437 - accuracy: 1.0000 - val_loss: 1.0029 - val_accuracy: 0.6667\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 1.1401 - val_accuracy: 0.6667\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 1.3489 - val_accuracy: 0.6667\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 1.5375 - val_accuracy: 0.3333\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.6854 - val_accuracy: 0.3333\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.8192 - val_accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "history_class = model_class.fit(padded_sequences, \n",
    "                                labels_cat, \n",
    "                                epochs=10, \n",
    "                                batch_size=4, \n",
    "                                validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WLV-AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
