{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9906f33",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/soham1024/basic-neural-network-from-scratch-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1736c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "from mathutils import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 6\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80384cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00214401, 0.0158422 , 0.11705891, 0.86495488]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "softmax(np.array([[2, 4, 6, 8]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75501beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60000           # Number of iterations\n",
    "inputLayerSize, hiddenLayerSize, outputLayerSize = 2, 3, 1\n",
    "LR = 0.1                 # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6005b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our data\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([ [0],   [1],   [1],   [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfa819bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights of our NN to random numbers :\n",
    "\n",
    "w_hidden = np.random.uniform(size=(inputLayerSize, hiddenLayerSize))\n",
    "w_output = np.random.uniform(size=(hiddenLayerSize,outputLayerSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5697e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the Backprop algorithm\n",
    "\n",
    "def sigmoid (x): return 1/(1 + np.exp(-x))           # activation function\n",
    "def sigmoid_prime(x): return x * (1 - x)             # derivative of sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "195b782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error sum [-1.77496016]\n",
      "error sum [0.02481767]\n",
      "error sum [0.01544889]\n",
      "error sum [0.01214582]\n",
      "error sum [0.01032507]\n",
      "error sum [0.00913186]\n",
      "error sum [0.00827274]\n",
      "error sum [0.00761624]\n",
      "error sum [0.00709348]\n",
      "error sum [0.00666447]\n",
      "error sum [0.00630417]\n",
      "error sum [0.00599601]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    " \n",
    "    # Forward\n",
    "    act_hidden = sigmoid(np.dot(X, w_hidden))\n",
    "    output = np.dot(act_hidden, w_output)\n",
    "    \n",
    "    # Calculate error\n",
    "    error = y - output\n",
    "    \n",
    "    if epoch % 5000 == 0:\n",
    "        print(f'error sum {sum(error)}')\n",
    "\n",
    "    # Backward\n",
    "    dZ = error * LR\n",
    "    w_output += act_hidden.T.dot(dZ)\n",
    "    dH = dZ.dot(w_output.T) * sigmoid_prime(act_hidden)\n",
    "    w_hidden += X.T.dot(dH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1937a309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X[1] # [0, 1]\n",
    "\n",
    "act_hidden = sigmoid(np.dot(X_test, w_hidden))\n",
    "np.round(np.dot(act_hidden, w_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7c78a",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network\n",
    "\n",
    "Feedforward Neural Network (FNN) is a type of artificial neural network in which information flows in a single direction i.e from the input layer through hidden layers to the output layer without loops or feedback. It is mainly used for pattern recognition tasks like image and speech classification.\n",
    "\n",
    "For example in a credit scoring system, banks use an FNN which analyze users financial profiles such as income, credit history and spending habits to determine their creditworthiness.\n",
    "\n",
    "**Theory:**\n",
    "- https://www.geeksforgeeks.org/deep-learning/feedforward-neural-network/\n",
    "- https://www.geeksforgeeks.org/deep-learning/deep-learning-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e43b9fc",
   "metadata": {},
   "source": [
    "#### Structure of a Feedforward Neural Network\n",
    "Feedforward Neural Networks have a structured layered design where data flows sequentially through each layer.\n",
    "\n",
    "1. **Input Layer:** The input layer consists of neurons that receive the input data. Each neuron in the input layer represents a feature of the input data.\n",
    "2. **Hidden Layers:** One or more hidden layers are placed between the input and output layers. These layers are responsible for learning the complex patterns in the data. Each neuron in a hidden layer applies a weighted sum of inputs followed by a non-linear activation function.\n",
    "3. **Output Layer:** The output layer provides the final output of the network. The number of neurons in this layer corresponds to the number of classes in a classification problem or the number of outputs in a regression problem.\n",
    "\n",
    "\n",
    "**Activation Functions**\n",
    "Activation functions introduce non-linearity into the network enabling it to learn and model complex data patterns.\n",
    "\n",
    "Common activation functions include:\n",
    "- **Sigmoid:** $  \\sigma(x) = \\frac{1}{1 + e^{-x}}  $\n",
    "- **Tanh:** $  \\tanh(x) = \\frac{e^{x}-e^{-x}}{e^{x} + e^{-x}}  $\n",
    "- **ReLu:** ReLu(x)=max(0,x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497926c",
   "metadata": {},
   "source": [
    "#### Training a Feedforward Neural Network\n",
    "Training a Feedforward Neural Network involves adjusting the weights of the neurons to minimize the error between the predicted output and the actual output. This process is typically performed using backpropagation and gradient descent.\n",
    "\n",
    "1. **Forward Propagation:** During forward propagation the input data passes through the network and the output is calculated.\n",
    "2. **Loss Calculation:** The loss (or error) is calculated using a loss function such as Mean Squared Error (MSE) for regression tasks or Cross-Entropy Loss for classification tasks.\n",
    "3. **Backpropagation:** In backpropagation the error is propagated back through the network to update the weights. The gradient of the loss function with respect to each weight is calculated and the weights are adjusted using gradient descent.\n",
    "\n",
    "**Note:** During training, a feedforward neural network performs forward pass followed by backpropagation to update weights, while during prediction only the forward pass is used. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042db962",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize the loss function by iteratively updating the weights in the direction of the negative gradient. Common variants of gradient descent include:\n",
    "\n",
    "- **Batch Gradient Descent:** Updates weights after computing the gradient over the entire dataset.\n",
    "- **Stochastic Gradient Descent (SGD):** Updates weights for each training example individually.\n",
    "- **Mini-batch Gradient Descent:** It Updates weights after computing the gradient over a small batch of training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69de183",
   "metadata": {},
   "source": [
    "#### Evaluation of Feedforward neural network\n",
    "Evaluating the performance of the trained model involves several metrics:\n",
    "\n",
    "- **Accuracy:** The proportion of correctly classified instances out of the total instances.\n",
    "- **Precision:** The ratio of true positive predictions to the total predicted positives.\n",
    "- **Recall:** The ratio of true positive predictions to the actual positives.\n",
    "- **F1 Score:** The harmonic mean of precision and recall, providing a balance between the two.\n",
    "- ***Confusion Matrix:** A table used to describe the performance of a classification model, showing the true positives, true negatives, false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e3b943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2623 - sparse_categorical_accuracy: 0.9253\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1180 - sparse_categorical_accuracy: 0.9653\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0805 - sparse_categorical_accuracy: 0.9764\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0597 - sparse_categorical_accuracy: 0.9816\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0453 - sparse_categorical_accuracy: 0.9862\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0751 - sparse_categorical_accuracy: 0.9779\n",
      "\n",
      "Test accuracy: 0.9779000282287598\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    This code demonstrates the process of building, training and evaluating a neural network model using TensorFlow and Keras to classify handwritten digits from the MNIST dataset.\n",
    "    The model architecture is defined using the Sequential consisting of:\n",
    "        - a Flatten layer to convert the 2D image input into a 1D array\n",
    "        - a Dense layer with 128 neurons and ReLU activation\n",
    "        - a final Dense layer with 10 neurons and softmax activation to output probabilities for each digit class.\n",
    "\n",
    "    Model is compiled with\n",
    "        - Adam optimizer\n",
    "        - Sparse Categorical Crossentropy loss function\n",
    "        - Sparse Categorical Accuracy metric\n",
    "        - Then trained for 5 epochs on the training data\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "# Load and prepare the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss=SparseCategoricalCrossentropy(),\n",
    "              metrics=[SparseCategoricalAccuracy()])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'\\nTest accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48bb4bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4: Micro-lecture - Hyper Parameters\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a6614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the MNIST dataset\n",
    "batch_size = 500\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_test, (-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cef623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserver 10000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52fe290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 20:56:06.538442: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-09 20:56:06.583527: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-09 20:56:06.583893: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-09 20:56:06.585245: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-09 20:56:06.585509: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-09 20:56:06.585749: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-09 20:56:06.658881: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-09 20:56:06.659081: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-09 20:56:06.659234: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-09 20:56:06.659346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3473 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Prepared the training dataset as tf.data.Dataset objects\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02c73f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TakeDataset element_spec=(TensorSpec(shape=(None, 784), dtype=tf.uint8, name=None), TensorSpec(shape=(None,), dtype=tf.uint8, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "print(train_dataset.take(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WLV-AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
